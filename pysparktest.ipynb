{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"appName\").master(\"local[*]\").getOrCreate()\n",
    "#from pyspark.sql import Row\n",
    "from pyspark.sql import HiveContext\n",
    "sqlc = HiveContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  1| 144.5|   5.9| 33|     罗|\n",
      "|  2| 167.2|   5.4| 45|     站|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hive 与 dataframe 之间的转换\n",
    "df = spark.createDataFrame([(1,144.5,5.9,33,'罗'),(2,167.2,5.4,45,'站'),(3,124.1,5.2,23,'神'),(4,144.5,5.9,33,'m'),(5,133.2,5.7,54,'f'),(3,124.1,5.2,23,'f'),(5,129.2,5.2,23,'m')],['id','weight','height','age','gender'])\n",
    "\n",
    "# 在hive 中创建相关的表。 \n",
    "sqlc.sql(\"use lk\")\n",
    "sqlc.sql(\"drop table if exists sxml_1013\")\n",
    "sqlc.sql(\"create table sxml_1013(id int,weight float,height float,age int,gender string ) row format delimited fields terminated by '\\t' lines terminated by '\\n'\")\n",
    "\n",
    "#然后将dataframe 的数据导入到hive中\n",
    "df.write.insertInto(\"sxml_1013\")  #成功\n",
    "\n",
    "# 从hive 中拿出来的数据是 dataframe型的数据。\n",
    "td = sqlc.sql(\"select * from sxml_1013 limit 2\")\n",
    "td.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  4| 144.5|   5.9| 33|     m|\n",
      "|  5| 129.2|   5.2| 23|     m|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''text_file = sc.textFile(\"hdfs://...\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "counts.saveAsTextFile(\"hdfs://...\")'''\n",
    "\n",
    "# sc =spark.sparkContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# 从hdfs中拿数据 回到hive中 或者hdfs中去\n",
    "data = sc.textFile(\"hdfs://luokuideMacBook-Pro.local:8020/user/hive/warehouse/lk.db/sxml_1013\")\n",
    "data = data.map(lambda r:r.split('\\t'))# sc 拿的是rdd\n",
    "\n",
    "# 对数据进行处理之后 转换成 dataframe\n",
    "names=\"id weight height age gender\"\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in names.split()]\n",
    "schema = StructType(fields)\n",
    "data = spark.createDataFrame(data,schema)\n",
    "\n",
    "\n",
    "'''与上面等价但是更灵活\n",
    "import pyspark.sql.types as typ\n",
    "labelx = [('INFANT_ALIVE_AT_REPORT',typ.IntegerType()),\n",
    "        ('BIRTH_PLACE',typ.StringType()),\n",
    "         ('MOTHER_WEIGHT_GAIN',typ.FloatType()),\n",
    "         ('PREV_BIRTH_PRETERM',typ.IntegerType())]\n",
    "shema = typ.StructType([typ.StructField(e[0],e[1],False) for e in labelx])'''\n",
    "\n",
    "\n",
    "# \n",
    "data.createOrReplaceTempView(\"people\")\n",
    "tempdata = spark.sql('select * from people where gender=\"m\"')\n",
    "tempdata.show() # 也是dataframe 型的\n",
    "\n",
    "#将 sql处理好的数据导入hive，然后hdfs中也会有类似的数据。\n",
    "sqlc.sql(\"drop table if exists u_data_416\")\n",
    "sqlc.sql(\"CREATE TABLE u_data_416 \\\n",
    "               (id INT,weight float,height float,age float,gender string) \\\n",
    "               ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\")\n",
    "tempdata.write.insertInto(\"u_data_416\")\n",
    "\n",
    "\n",
    "# 将detaframe 转换成rdd 之后再存入 hdfs中去就可以了。但是hive中是没有数据的\n",
    "temp = tempdata.rdd.map(lambda row:[e for e in row])\n",
    "#temp.take(2)\n",
    "temp.saveAsTextFile(\"hdfs://luokuideMacBook-Pro.local:8020/user/hive/warehouse/lk.db/data_416_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global_temp的用法\n",
    "df.createGlobalTempView(\"people\")\n",
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n",
    "\n",
    "# 对 rdd数据的查询\n",
    "from pyspark.sql import Row\n",
    "sc = spark.sparkContext\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"examples/src/main/resources/people.txt\") #外部表\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1]))) #注意列名\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "for name in teenNames:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "读取其他格式的数据集\n",
    "\n",
    "df = spark.read.load(\"examples/src/main/resources/users.parquet\")\n",
    "df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")\n",
    "\n",
    "df = spark.read.load(\"examples/src/main/resources/people.json\", format=\"json\")\n",
    "df.select(\"name\", \"age\").write.save(\"namesAndAges.parquet\", format=\"parquet\")\n",
    "\n",
    "df = spark.read.load(\"examples/src/main/resources/people.csv\",\n",
    "                     format=\"csv\", sep=\":\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "df = spark.sql(\"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "def multiply_func(a, b):\n",
    "    return a * b\n",
    "\n",
    "multiply = pandas_udf(multiply_func, returnType=LongType())\n",
    "# The function for a pandas_udf should be able to execute with local Pandas data\n",
    "x = pd.Series([1, 2, 3])\n",
    "print(multiply_func(x, x))\n",
    "# 0    1\n",
    "# Create a Spark DataFrame, 'spark' is an existing SparkSession\n",
    "df = spark.createDataFrame(pd.DataFrame(x, columns=[\"x\"]))\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(multiply(col(\"x\"), col(\"x\"))).show()\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))\n",
    "\n",
    "@pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)\n",
    "def substract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=v - v.mean())\n",
    "\n",
    "df.groupby(\"id\").apply(substract_mean).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "textFile() got an unexpected keyword argument 'fieldDelim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-206776dd7bbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://luokuideMacBook-Pro.local:8020                    /user/hive/warehouse/lk.db/sxml_1013\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfieldDelim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: textFile() got an unexpected keyword argument 'fieldDelim'"
     ]
    }
   ],
   "source": [
    "data = sc.textFile(\"hdfs://luokuideMacBook-Pro.local:8020 \\\n",
    "                   /user/hive/warehouse/lk.db/sxml_1013\",fieldDelim='\\t')\n",
    "data.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of rows:7\n",
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  1| 144.5|   5.9| 33|     m|\n",
      "|  2| 167.2|   5.4| 45|     m|\n",
      "|  3| 124.1|   5.2| 23|     f|\n",
      "|  4| 144.5|   5.9| 33|     m|\n",
      "|  5| 133.2|   5.7| 54|     f|\n",
      "|  3| 124.1|   5.2| 23|     f|\n",
      "|  5| 129.2|   5.2| 23|     m|\n",
      "+---+------+------+---+------+\n",
      "\n",
      "None\n",
      "[Row(id=1, weight=144.5, height=5.9, age=33, gender='m'), Row(id=2, weight=167.2, height=5.4, age=45, gender='m')]\n"
     ]
    }
   ],
   "source": [
    "print('count of rows:{0}'.format(df.count()))\n",
    "#show 和 take 在dataframe 上用法的差异\n",
    "#注意产看是否有重复行的数据\n",
    "print('count of distinct rows{0}'.format(df.distinct().count()))\n",
    "# 进行去重\n",
    "df =df.dropDuplicates() \n",
    "\n",
    "print(\"Count if ids:{0}\".format(df.select([c for c in df.columns if c!='id']).distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = df.select([c for c in df.columns if c!='id']) # 如何筛选列数据？\n",
    "df11 = df.select([c for c in df.columns if 'age' and 'id' not in c])  \n",
    "# 注意这里的用发 ，巧妙啊 利用 and 可以减少个数 and 和& 在不同场景下的使用\n",
    "df12 = df.select([c for c in df.columns if 'e' not in c])  #注意这里的用法\n",
    "#df2 = df1.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  3| 124.1|   5.2| 23|     f|\n",
      "|  5| 129.2|   5.2| 23|     m|\n",
      "|  5| 133.2|   5.7| 54|     f|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df.dropDuplicates(subset=[c for c in df.columns if 'id' not in c]) # 利用subset 进行筛选 筛选，行的方法\n",
    "# 根据条件去重。\n",
    "df3.where('height>5 and weight<142').show() # 查看 符合某种条件的 数据 满足多种条件的筛选方法 \n",
    "# where 的用发\n",
    "# df_miss.where('id<2 or id>2').show()\n",
    "#df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|count|distinct|\n",
      "+-----+--------+\n",
      "|    5|       4|\n",
      "+-----+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "df3.agg(fn.count('id').alias('count'),fn.countDistinct('id').alias('distinct')).show()  # countDistinct\n",
    "# functions ，cout() ,alias() 主要是agg的用法 ，这里是合一\n",
    "df3.select('id').count()  #这里不需要用show() 注意与上面方法的差异   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  3| 124.1|   5.2| 23|     f|\n",
      "|  5| 129.2|   5.2| 23|     m|\n",
      "|  4| 144.5|   5.9| 33|     m|\n",
      "|  5| 133.2|   5.7| 54|     f|\n",
      "|  2| 167.2|   5.4| 45|     m|\n",
      "+---+------+------+---+------+\n",
      "\n",
      "+---+\n",
      "|  c|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#df3.withColumn('new_id',fn.monotonically_increasing_id()).show() #应该用处不大\n",
    "df3.show()\n",
    "b = spark.createDataFrame([(1,2,3,4,5),(2,3,4,5,6)],['b','c']) #为什么只能创建行呢？ 列呢？\n",
    "c = spark.createDataFrame([([e]) for e in range(5)],['c'])  # 如何创建列ne ? 【 ([e]) for e in range(5)】\n",
    "#print(c.show())\n",
    "#df3.withColumn('n_id',pd.DataFrame(a)).show()  #那 如何添加一列数据那边哦？\n",
    "# df4 = pd.concat([df3,c],axis=1)\n",
    "#df4.show()\n",
    "print(c.show())\n",
    "# 如何查看矩阵维度\n",
    "#print(df3.describe)\n",
    "#print(df3.count()) print(len(df3.first())) 产看dataframe的 维度的方法 count() len(df.first())\n",
    "# df3.withColumn('c',c).show() #追加 一列失败\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 如何 查看数据的 矩阵呢？\n",
    "df_miss = spark.createDataFrame([(1,None,1,None,'m',1000),(2,3,33,1,'f',None),(3,4,None,5,'m',200)],['id','a','b','c','gen','d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+---+----+\n",
      "| id|   a|   b|   c|gen|   d|\n",
      "+---+----+----+----+---+----+\n",
      "|  1|null|   1|null|  m|1000|\n",
      "|  2|   3|  33|   1|  f|null|\n",
      "|  3|   4|null|   5|  m| 200|\n",
      "+---+----+----+----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 1), (3, 1)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_miss.rdd.map(lambda row : ( row ['id'],sum([c==None for c in row]))).collect() #如何合并数据呢？\n",
    "# 计算每行中null的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+---+---+---+\n",
      "| id|  a|   b|  c|gen|  d|\n",
      "+---+---+----+---+---+---+\n",
      "|  3|  4|null|  5|  m|200|\n",
      "+---+---+----+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss.where('b is null').show() # 查看 符合某种条件的 数据 注意这里的用法 a is null\n",
    "# df_miss.where('id<2 or id>2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+-----------+-------------------+\n",
      "|id_missing|          a_missing|          b_missing|          c_missing|gen_missing|          d_missing|\n",
      "+----------+-------------------+-------------------+-------------------+-----------+-------------------+\n",
      "|       0.0|0.33333333333333337|0.33333333333333337|0.33333333333333337|        0.0|0.33333333333333337|\n",
      "+----------+-------------------+-------------------+-------------------+-----------+-------------------+\n",
      "\n",
      "+------------------+-----------------+-----------------+-----------------+-------------------+-----------------+\n",
      "|count(DISTINCT id)|count(DISTINCT a)|count(DISTINCT b)|count(DISTINCT c)|count(DISTINCT gen)|count(DISTINCT d)|\n",
      "+------------------+-----------------+-----------------+-----------------+-------------------+-----------------+\n",
      "|                 3|                2|                2|                2|                  2|                2|\n",
      "+------------------+-----------------+-----------------+-----------------+-------------------+-----------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_miss.agg(*[(1-(fn.count(c)/fn.count('*'))).alias(c+'_missing') for c in df_miss.columns]).show() #xing hao\n",
    "print(df_miss.agg(*[fn.countDistinct(c) for c in df_miss.columns]).show())\n",
    "# 注意 fn.countDistinct(c) 的用法  df.distinct().count() 的区别与联系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+---+----+\n",
      "| id|   a|   b|   c|gen|   d|\n",
      "+---+----+----+----+---+----+\n",
      "|  1|null|   1|null|  m|1000|\n",
      "|  2|   3|  33|   1|  f|null|\n",
      "|  3|   4|null|   5|  m| 200|\n",
      "+---+----+----+----+---+----+\n",
      "\n",
      "+---+----+----+----+---+----+\n",
      "| id|   a|   b|   c|gen|   d|\n",
      "+---+----+----+----+---+----+\n",
      "|  1|null|   1|null|  m|1000|\n",
      "|  2|   3|  33|   1|  f|null|\n",
      "|  3|   4|null|   5|  m| 200|\n",
      "+---+----+----+----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss.dropna(thresh=1).show()  # thresh 没有用嘛？\n",
    "df_miss.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import tensorframs as tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('INFANT_ALIVE_AT_REPORT', StringType), ('BIRTH_YEAR', IntegerType), ('BIRTH_PLACE', StringType), ('MOTHER_AGE_YEARS', IntegerType), ('FATHER_COMBINED_AGE', IntegerType), ('CIG_BEFORE', StringType), ('CIG_1_TRI', IntegerType), ('CIG_2_TRI', IntegerType), ('CIG_3_TRI', IntegerType), ('MOTHER_HEIGHT_IN', FloatType), ('MOTHER_PRE_WEIGHT', FloatType), ('MOTHER_DELIVERY_WEIGHT', FloatType), ('MOTHER_WEIGHT_GAIN', FloatType), ('DIABETES_PRE', FloatType), ('DIABETES_GEST', FloatType), ('HYP_TENS_PRE', FloatType), ('HYP_TENS_GEST', FloatType), ('PREV_BIRTH_PRETERM', FloatType)]\n",
      "StructType(List(StructField(INFANT_ALIVE_AT_REPORT,StringType,false),StructField(BIRTH_YEAR,IntegerType,false),StructField(BIRTH_PLACE,StringType,false),StructField(MOTHER_AGE_YEARS,IntegerType,false),StructField(FATHER_COMBINED_AGE,IntegerType,false),StructField(CIG_BEFORE,StringType,false),StructField(CIG_1_TRI,IntegerType,false),StructField(CIG_2_TRI,IntegerType,false),StructField(CIG_3_TRI,IntegerType,false),StructField(MOTHER_HEIGHT_IN,FloatType,false),StructField(MOTHER_PRE_WEIGHT,FloatType,false),StructField(MOTHER_DELIVERY_WEIGHT,FloatType,false),StructField(MOTHER_WEIGHT_GAIN,FloatType,false),StructField(DIABETES_PRE,FloatType,false),StructField(DIABETES_GEST,FloatType,false),StructField(HYP_TENS_PRE,FloatType,false),StructField(HYP_TENS_GEST,FloatType,false),StructField(PREV_BIRTH_PRETERM,FloatType,false)))\n"
     ]
    }
   ],
   "source": [
    "labels = [('INFANT_ALIVE_AT_REPORT',typ.StringType()),\n",
    "        ('BIRTH_YEAR',typ.IntegerType()),\n",
    "        ('BIRTH_PLACE',typ.StringType()),\n",
    "        ('MOTHER_AGE_YEARS',typ.IntegerType()),\n",
    "        ('FATHER_COMBINED_AGE',typ.IntegerType()),\n",
    "        ('CIG_BEFORE',typ.StringType()),\n",
    "        ('CIG_1_TRI',typ.IntegerType()),\n",
    "        ('CIG_2_TRI',typ.IntegerType()),\n",
    "        ('CIG_3_TRI',typ.IntegerType()),\n",
    "         ('MOTHER_HEIGHT_IN',typ.FloatType()),\n",
    "         ('MOTHER_PRE_WEIGHT',typ.FloatType()),\n",
    "         ('MOTHER_DELIVERY_WEIGHT',typ.FloatType()),\n",
    "         ('MOTHER_WEIGHT_GAIN',typ.FloatType()),\n",
    "         ('DIABETES_PRE',typ.FloatType()),\n",
    "         ('DIABETES_GEST',typ.FloatType()),\n",
    "         ('HYP_TENS_PRE',typ.FloatType()),\n",
    "         ('HYP_TENS_GEST',typ.FloatType()),\n",
    "         ('PREV_BIRTH_PRETERM',typ.FloatType())]\n",
    "print(labels)\n",
    "schema=typ.StructType([typ.StructField(e[0],e[1],False) for e in labels])\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "births =spark.read.csv(\"file:///home/fordl/births_train.csv.gz\",header=True)#,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#births.show()\n",
    "recode_dictionary={\n",
    "    'YUN':{\n",
    "        'Y':1,\n",
    "        'N':0,\n",
    "        'U':0\n",
    "    }\n",
    "}\n",
    "#print(recode_dictionary['YUN']['N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_features=['INFANT_ALIVE_AT_REPORT','BIRTH_YEAR','BIRTH_PLACE','MOTHER_AGE_YEARS',\n",
    "                   'FATHER_COMBINED_AGE','CIG_BEFORE', 'CIG_1_TRI','CIG_2_TRI','CIG_3_TRI','MOTHER_HEIGHT_IN',\n",
    "                   'MOTHER_PRE_WEIGHT','MOTHER_DELIVERY_WEIGHT','MOTHER_WEIGHT_GAIN',\n",
    "                   'DIABETES_PRE','DIABETES_GEST','HYP_TENS_PRE','HYP_TENS_GEST','PREV_BIRTH_PRETERM']\n",
    "births_trimmed = births.select(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45429"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#births_trimmed.select(\"BIRTH_YEAR\").where('BIRTH_YEAR<2015').show()\n",
    "# df3.where('height>5 and weight<142').show() # 查看 符合某种条件的 数据 满足多种条件的筛选方法  多种数据的筛选方法\n",
    "births_trimmed.count()  #如何进行 group by 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "def recode(col,key):\n",
    "    return recode_dictionary[key][col]\n",
    "def correct_cig(feat):\n",
    "    return func.when(func.col(feat)!=99,func.col(feat)).otherwise(0) \n",
    "#这里的用法注意的 func.when(func.col(feat)!=99.func.col(feat))\n",
    "rec_integer = func.udf(recode,typ.IntegerType()) #调用 func.udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|CIG_BEFORE|CIG_1_TRI|\n",
      "+----------+---------+\n",
      "|        99|       99|\n",
      "+----------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n",
      "+---------+---------+\n",
      "|CIG_1_TRI|CIG_2_TRI|\n",
      "+---------+---------+\n",
      "|        0|        0|\n",
      "+---------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(births_trimmed.select('CIG_BEFORE','CIG_1_TRI').show(1)) \n",
    "# 利用head 来提取数据就好了，那么如何提取前面几个数字呢\n",
    "# HEAD ： DATA.HEAD（）  或者 data.show(3),再或者如上所示。\n",
    "births_transformed=births_trimmed.withColumn('CIG_BEFORE',correct_cig('CIG_BEFORE'))\\\n",
    ".withColumn('CIG_1_TRI',correct_cig('CIG_1_TRI'))\\\n",
    ".withColumn('CIG_2_TRI',correct_cig('CIG_2_TRI'))\\\n",
    ".withColumn('CIG_3_TRI',correct_cig('CIG_3_TRI'))\n",
    "print(births_transformed.select(\"CIG_1_TRI\",\"CIG_2_TRI\").show(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(INFANT_ALIVE_AT_REPORT,StringType,true),StructField(BIRTH_YEAR,StringType,true),StructField(BIRTH_PLACE,StringType,true),StructField(MOTHER_AGE_YEARS,StringType,true),StructField(FATHER_COMBINED_AGE,StringType,true),StructField(CIG_BEFORE,StringType,true),StructField(CIG_1_TRI,StringType,true),StructField(CIG_2_TRI,StringType,true),StructField(CIG_3_TRI,StringType,true),StructField(MOTHER_HEIGHT_IN,StringType,true),StructField(MOTHER_PRE_WEIGHT,StringType,true),StructField(MOTHER_DELIVERY_WEIGHT,StringType,true),StructField(MOTHER_WEIGHT_GAIN,StringType,true),StructField(DIABETES_PRE,StringType,true),StructField(DIABETES_GEST,StringType,true),StructField(HYP_TENS_PRE,StringType,true),StructField(HYP_TENS_GEST,StringType,true),StructField(PREV_BIRTH_PRETERM,StringType,true)))\n",
      "('INFANT_ALIVE_AT_REPORT', StringType)\n",
      "('BIRTH_YEAR', StringType)\n"
     ]
    }
   ],
   "source": [
    "print(births_trimmed.schema) #应该是对列名的描shu\n",
    "cols=[(col.name,col.dataType) for col in births_trimmed.schema]\n",
    "print(cols[0]) #提取对应的列名 及 其属性。\n",
    "print(cols[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INFANT_ALIVE_AT_REPORT', 'DIABETES_PRE', 'DIABETES_GEST', 'HYP_TENS_PRE', 'HYP_TENS_GEST', 'PREV_BIRTH_PRETERM']\n"
     ]
    }
   ],
   "source": [
    "YUN_cols=[]\n",
    "for i,s in enumerate(cols):\n",
    "    if s[1]==typ.StringType():\n",
    "        #print(s[0],\"****\")\n",
    "        dis =births.select(s[0]).distinct().rdd.map(lambda row:row[0]).collect() #这里是什么意思？\n",
    "        #print(dis,\"____\")\n",
    "    if 'Y' in dis:\n",
    "        YUN_cols.append(s[0])\n",
    "print(YUN_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2014', '2015']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "births.select(cols[1][0]).distinct().rdd.map(lambda row:row[0]).collect() #用来查看每个一个zhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(BIRTH_YEAR='2015'),\n",
       " Row(BIRTH_YEAR='2015'),\n",
       " Row(BIRTH_YEAR='2015'),\n",
       " Row(BIRTH_YEAR='2015'),\n",
       " Row(BIRTH_YEAR='2015')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "births_trimmed.select('BIRTH_YEAR').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(INFANT_NICU_ADMISSION='Y', INFANT_NICU_ADMISSION_RECODE=1),\n",
       " Row(INFANT_NICU_ADMISSION='Y', INFANT_NICU_ADMISSION_RECODE=1),\n",
       " Row(INFANT_NICU_ADMISSION='U', INFANT_NICU_ADMISSION_RECODE=0),\n",
       " Row(INFANT_NICU_ADMISSION='N', INFANT_NICU_ADMISSION_RECODE=0),\n",
       " Row(INFANT_NICU_ADMISSION='U', INFANT_NICU_ADMISSION_RECODE=0)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birthss =spark.read.csv(\"file:///home/fordl/births_train.csv.gz\",header=True) #注意这个梗\n",
    "birthss.select(['INFANT_NICU_ADMISSION',rec_integer('INFANT_NICU_ADMISSION',func.lit('YUN')).alias('INFANT_NICU_ADMISSION_RECODE')]).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0),\n",
       " Row(DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0),\n",
       " Row(DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exprs_YNU=[rec_integer(x,func.lit('YUN')).alias(x)\n",
    "          if x in YUN_cols\n",
    "          else x\n",
    "          for x in births_transformed.columns]\n",
    "births_transformed = births_transformed.select(exprs_YNU)\n",
    "births_transformed.select(YUN_cols[-5:]).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOTHER_AGE_YEARS:\t28.30\t6.08\n",
      "FATHER_COMBINED_AGE:\t44.55\t27.55\n",
      "CIG_BEFORE:\t1.43\t5.18\n",
      "CIG_1_TRI:\t0.91\t3.83\n",
      "CIG_2_TRI:\t0.70\t3.31\n",
      "CIG_3_TRI:\t0.58\t3.11\n",
      "MOTHER_HEIGHT_IN:\t65.12\t6.45\n",
      "MOTHER_PRE_WEIGHT:\t214.50\t210.21\n",
      "MOTHER_DELIVERY_WEIGHT:\t223.63\t180.01\n",
      "MOTHER_WEIGHT_GAIN:\t30.74\t26.23\n"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.stat as st\n",
    "import numpy as np\n",
    "numeric_cols=['MOTHER_AGE_YEARS','FATHER_COMBINED_AGE','CIG_BEFORE','CIG_1_TRI','CIG_2_TRI','CIG_3_TRI',\n",
    "             'MOTHER_HEIGHT_IN','MOTHER_PRE_WEIGHT','MOTHER_DELIVERY_WEIGHT','MOTHER_WEIGHT_GAIN']\n",
    "numeric_rdd = births_transformed.select(numeric_cols).rdd.map(lambda row : [e for e in row])\n",
    "mllib_stats = st.Statistics.colStats(numeric_rdd)\n",
    "for col,m,v in zip(numeric_cols,mllib_stats.mean(),mllib_stats.variance()):\n",
    "    print('{0}:\\t{1:.2f}\\t{2:.2f}'.format(col,m,np.sqrt(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFANT_ALIVE_AT_REPORT [(1, 23349), (0, 22080)]\n",
      "BIRTH_YEAR [('2014', 22842), ('2015', 22587)]\n",
      "BIRTH_PLACE [('1', 44558), ('4', 327), ('3', 224), ('2', 136), ('7', 91), ('5', 74), ('6', 11), ('9', 8)]\n",
      "DIABETES_PRE [(0, 44881), (1, 548)]\n",
      "DIABETES_GEST [(0, 43451), (1, 1978)]\n",
      "HYP_TENS_PRE [(0, 44348), (1, 1081)]\n",
      "HYP_TENS_GEST [(0, 43302), (1, 2127)]\n",
      "PREV_BIRTH_PRETERM [(0, 43088), (1, 2341)]\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = [e for e in births_transformed.columns if e not in numeric_cols]\n",
    "categorical_rdd = births_transformed.select(categorical_cols).rdd.map(lambda row: [e for e in row])\n",
    "for i ,col in enumerate(categorical_cols):\n",
    "    agg =categorical_rdd.groupBy(lambda row: row[i]).map(lambda row:(row[0],len(row[1])))\n",
    "    print(col,sorted(agg.collect(),key=lambda el:el[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIG_BEFORE-to-CIG_1_TRI:0.83\n",
      "CIG_BEFORE-to-CIG_2_TRI:0.72\n",
      "CIG_BEFORE-to-CIG_3_TRI:0.62\n",
      "CIG_1_TRI-to-CIG_BEFORE:0.83\n",
      "CIG_1_TRI-to-CIG_2_TRI:0.87\n",
      "CIG_1_TRI-to-CIG_3_TRI:0.76\n",
      "CIG_2_TRI-to-CIG_BEFORE:0.72\n",
      "CIG_2_TRI-to-CIG_1_TRI:0.87\n",
      "CIG_2_TRI-to-CIG_3_TRI:0.89\n",
      "CIG_3_TRI-to-CIG_BEFORE:0.62\n",
      "CIG_3_TRI-to-CIG_1_TRI:0.76\n",
      "CIG_3_TRI-to-CIG_2_TRI:0.89\n",
      "MOTHER_PRE_WEIGHT-to-MOTHER_DELIVERY_WEIGHT:0.54\n",
      "MOTHER_PRE_WEIGHT-to-MOTHER_WEIGHT_GAIN:0.65\n",
      "MOTHER_DELIVERY_WEIGHT-to-MOTHER_PRE_WEIGHT:0.54\n",
      "MOTHER_DELIVERY_WEIGHT-to-MOTHER_WEIGHT_GAIN:0.60\n",
      "MOTHER_WEIGHT_GAIN-to-MOTHER_PRE_WEIGHT:0.65\n",
      "MOTHER_WEIGHT_GAIN-to-MOTHER_DELIVERY_WEIGHT:0.60\n"
     ]
    }
   ],
   "source": [
    "corrs = st.Statistics.corr(numeric_rdd)\n",
    "for i,el in enumerate(corrs>0.5):\n",
    "    correlated = [\n",
    "        (numeric_cols[j],corrs[i][j])\n",
    "        for j ,e in enumerate(el)\n",
    "        if e==1.0 and j!=i]\n",
    "    if len(correlated)>0:\n",
    "        for e in correlated:\n",
    "            print('{0}-to-{1}:{2:.2f}'.format(numeric_cols[i],e[0],e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_keep=['INFANT_ALIVE_AT_REPORT',\n",
    "                 'BIRTH_PLACE',\n",
    "                 'MOTHER_AGE_YEARS',\n",
    "                 'FATHER_COMBINED_AGE',\n",
    "                 'CIG_1_TRI',\n",
    "                 'MOTHER_HEIGHT_IN',\n",
    "                 'MOTHER_PRE_WEIGHT',\n",
    "                 'DIABETES_PRE',\n",
    "                 'DIABETES_GEST',\n",
    "                 'HYP_TENS_GEST',\n",
    "                 'HYP_TENS_PRE',\n",
    "                 'PREV_BIRTH_PRETERM']\n",
    "births_transformed=births_transformed.select([e for e in features_to_keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`BIRTH_YEAR`' given input columns: [MOTHER_PRE_WEIGHT, MOTHER_AGE_YEARS, HYP_TENS_GEST, MOTHER_HEIGHT_IN, DIABETES_GEST, CIG_1_TRI, BIRTH_PLACE, INFANT_ALIVE_AT_REPORT, PREV_BIRTH_PRETERM, FATHER_COMBINED_AGE, HYP_TENS_PRE, DIABETES_PRE];;\\n'Project ['BIRTH_YEAR]\\n+- Project [INFANT_ALIVE_AT_REPORT#1331, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_1_TRI#1046, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, DIABETES_PRE#1332, DIABETES_GEST#1333, HYP_TENS_GEST#1335, HYP_TENS_PRE#1334, PREV_BIRTH_PRETERM#1336]\\n   +- Project [recode(INFANT_ALIVE_AT_REPORT#864, YUN) AS INFANT_ALIVE_AT_REPORT#1331, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_BEFORE#1026, CIG_1_TRI#1046, CIG_2_TRI#1066, CIG_3_TRI#1086, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, recode(DIABETES_PRE#883, YUN) AS DIABETES_PRE#1332, recode(DIABETES_GEST#884, YUN) AS DIABETES_GEST#1333, recode(HYP_TENS_PRE#885, YUN) AS HYP_TENS_PRE#1334, recode(HYP_TENS_GEST#886, YUN) AS HYP_TENS_GEST#1335, recode(PREV_BIRTH_PRETERM#887, YUN) AS PREV_BIRTH_PRETERM#1336]\\n      +- Project [INFANT_ALIVE_AT_REPORT#864, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_BEFORE#1026, CIG_1_TRI#1046, CIG_2_TRI#1066, CASE WHEN NOT (cast(CIG_3_TRI#877 as int) = 99) THEN CIG_3_TRI#877 ELSE cast(0 as string) END AS CIG_3_TRI#1086, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, DIABETES_PRE#883, DIABETES_GEST#884, HYP_TENS_PRE#885, HYP_TENS_GEST#886, PREV_BIRTH_PRETERM#887]\\n         +- Project [INFANT_ALIVE_AT_REPORT#864, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_BEFORE#1026, CIG_1_TRI#1046, CASE WHEN NOT (cast(CIG_2_TRI#876 as int) = 99) THEN CIG_2_TRI#876 ELSE cast(0 as string) END AS CIG_2_TRI#1066, CIG_3_TRI#877, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, DIABETES_PRE#883, DIABETES_GEST#884, HYP_TENS_PRE#885, HYP_TENS_GEST#886, PREV_BIRTH_PRETERM#887]\\n            +- Project [INFANT_ALIVE_AT_REPORT#864, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_BEFORE#1026, CASE WHEN NOT (cast(CIG_1_TRI#875 as int) = 99) THEN CIG_1_TRI#875 ELSE cast(0 as string) END AS CIG_1_TRI#1046, CIG_2_TRI#876, CIG_3_TRI#877, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, DIABETES_PRE#883, DIABETES_GEST#884, HYP_TENS_PRE#885, HYP_TENS_GEST#886, PREV_BIRTH_PRETERM#887]\\n               +- Project [INFANT_ALIVE_AT_REPORT#864, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CASE WHEN NOT (cast(CIG_BEFORE#874 as int) = 99) THEN CIG_BEFORE#874 ELSE cast(0 as string) END AS CIG_BEFORE#1026, CIG_1_TRI#875, CIG_2_TRI#876, CIG_3_TRI#877, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, DIABETES_PRE#883, DIABETES_GEST#884, HYP_TENS_PRE#885, HYP_TENS_GEST#886, PREV_BIRTH_PRETERM#887]\\n                  +- Project [INFANT_ALIVE_AT_REPORT#864, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_BEFORE#874, CIG_1_TRI#875, CIG_2_TRI#876, CIG_3_TRI#877, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, DIABETES_PRE#883, DIABETES_GEST#884, HYP_TENS_PRE#885, HYP_TENS_GEST#886, PREV_BIRTH_PRETERM#887]\\n                     +- Relation[INFANT_ALIVE_AT_REPORT#864,BIRTH_YEAR#865,BIRTH_MONTH#866,BIRTH_PLACE#867,MOTHER_AGE_YEARS#868,MOTHER_RACE_6CODE#869,MOTHER_EDUCATION#870,FATHER_COMBINED_AGE#871,FATHER_EDUCATION#872,MONTH_PRECARE_RECODE#873,CIG_BEFORE#874,CIG_1_TRI#875,CIG_2_TRI#876,CIG_3_TRI#877,MOTHER_HEIGHT_IN#878,MOTHER_BMI_RECODE#879,MOTHER_PRE_WEIGHT#880,MOTHER_DELIVERY_WEIGHT#881,MOTHER_WEIGHT_GAIN#882,DIABETES_PRE#883,DIABETES_GEST#884,HYP_TENS_PRE#885,HYP_TENS_GEST#886,PREV_BIRTH_PRETERM#887,... 30 more fields] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/bigdata/spark22/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/spark22/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1726.pivot.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`BIRTH_YEAR`' given input columns: [MOTHER_PRE_WEIGHT, MOTHER_AGE_YEARS, HYP_TENS_GEST, MOTHER_HEIGHT_IN, DIABETES_GEST, CIG_1_TRI, BIRTH_PLACE, INFANT_ALIVE_AT_REPORT, PREV_BIRTH_PRETERM, FATHER_COMBINED_AGE, HYP_TENS_PRE, DIABETES_PRE];;\n'Project ['BIRTH_YEAR]\n+- Project [INFANT_ALIVE_AT_REPORT#1331, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_1_TRI#1046, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, DIABETES_PRE#1332, DIABETES_GEST#1333, HYP_TENS_GEST#1335, HYP_TENS_PRE#1334, PREV_BIRTH_PRETERM#1336]\n   +- Project [recode(INFANT_ALIVE_AT_REPORT#864, YUN) AS INFANT_ALIVE_AT_REPORT#1331, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_BEFORE#1026, CIG_1_TRI#1046, CIG_2_TRI#1066, CIG_3_TRI#1086, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, recode(DIABETES_PRE#883, YUN) AS DIABETES_PRE#1332, recode(DIABETES_GEST#884, YUN) AS DIABETES_GEST#1333, recode(HYP_TENS_PRE#885, YUN) AS HYP_TENS_PRE#1334, recode(HYP_TENS_GEST#886, YUN) AS HYP_TENS_GEST#1335, recode(PREV_BIRTH_PRETERM#887, YUN) AS PREV_BIRTH_PRETERM#1336]\n      +- Project [INFANT_ALIVE_AT_REPORT#864, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_BEFORE#1026, CIG_1_TRI#1046, CIG_2_TRI#1066, CASE WHEN NOT (cast(CIG_3_TRI#877 as int) = 99) THEN CIG_3_TRI#877 ELSE cast(0 as string) END AS CIG_3_TRI#1086, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, DIABETES_PRE#883, DIABETES_GEST#884, HYP_TENS_PRE#885, HYP_TENS_GEST#886, PREV_BIRTH_PRETERM#887]\n         +- Project [INFANT_ALIVE_AT_REPORT#864, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_BEFORE#1026, CIG_1_TRI#1046, CASE WHEN NOT (cast(CIG_2_TRI#876 as int) = 99) THEN CIG_2_TRI#876 ELSE cast(0 as string) END AS CIG_2_TRI#1066, CIG_3_TRI#877, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, DIABETES_PRE#883, DIABETES_GEST#884, HYP_TENS_PRE#885, HYP_TENS_GEST#886, PREV_BIRTH_PRETERM#887]\n            +- Project [INFANT_ALIVE_AT_REPORT#864, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_BEFORE#1026, CASE WHEN NOT (cast(CIG_1_TRI#875 as int) = 99) THEN CIG_1_TRI#875 ELSE cast(0 as string) END AS CIG_1_TRI#1046, CIG_2_TRI#876, CIG_3_TRI#877, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, DIABETES_PRE#883, DIABETES_GEST#884, HYP_TENS_PRE#885, HYP_TENS_GEST#886, PREV_BIRTH_PRETERM#887]\n               +- Project [INFANT_ALIVE_AT_REPORT#864, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CASE WHEN NOT (cast(CIG_BEFORE#874 as int) = 99) THEN CIG_BEFORE#874 ELSE cast(0 as string) END AS CIG_BEFORE#1026, CIG_1_TRI#875, CIG_2_TRI#876, CIG_3_TRI#877, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, DIABETES_PRE#883, DIABETES_GEST#884, HYP_TENS_PRE#885, HYP_TENS_GEST#886, PREV_BIRTH_PRETERM#887]\n                  +- Project [INFANT_ALIVE_AT_REPORT#864, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_BEFORE#874, CIG_1_TRI#875, CIG_2_TRI#876, CIG_3_TRI#877, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, DIABETES_PRE#883, DIABETES_GEST#884, HYP_TENS_PRE#885, HYP_TENS_GEST#886, PREV_BIRTH_PRETERM#887]\n                     +- Relation[INFANT_ALIVE_AT_REPORT#864,BIRTH_YEAR#865,BIRTH_MONTH#866,BIRTH_PLACE#867,MOTHER_AGE_YEARS#868,MOTHER_RACE_6CODE#869,MOTHER_EDUCATION#870,FATHER_COMBINED_AGE#871,FATHER_EDUCATION#872,MONTH_PRECARE_RECODE#873,CIG_BEFORE#874,CIG_1_TRI#875,CIG_2_TRI#876,CIG_3_TRI#877,MOTHER_HEIGHT_IN#878,MOTHER_BMI_RECODE#879,MOTHER_PRE_WEIGHT#880,MOTHER_DELIVERY_WEIGHT#881,MOTHER_WEIGHT_GAIN#882,DIABETES_PRE#883,DIABETES_GEST#884,HYP_TENS_PRE#885,HYP_TENS_GEST#886,PREV_BIRTH_PRETERM#887,... 30 more fields] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2872)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1153)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1171)\n\tat org.apache.spark.sql.RelationalGroupedDataset.pivot(RelationalGroupedDataset.scala:320)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-5be8907b2fe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mln\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategorical_cols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0magg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbirths_transformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'INFANT_ALIVE_AT_REPORT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0magg_rdd\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/spark22/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36mpivot\u001b[0;34m(self, pivot_col, values)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mjgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpivot_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mjgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpivot_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/spark22/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/spark22/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`BIRTH_YEAR`' given input columns: [MOTHER_PRE_WEIGHT, MOTHER_AGE_YEARS, HYP_TENS_GEST, MOTHER_HEIGHT_IN, DIABETES_GEST, CIG_1_TRI, BIRTH_PLACE, INFANT_ALIVE_AT_REPORT, PREV_BIRTH_PRETERM, FATHER_COMBINED_AGE, HYP_TENS_PRE, DIABETES_PRE];;\\n'Project ['BIRTH_YEAR]\\n+- Project [INFANT_ALIVE_AT_REPORT#1331, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_1_TRI#1046, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, DIABETES_PRE#1332, DIABETES_GEST#1333, HYP_TENS_GEST#1335, HYP_TENS_PRE#1334, PREV_BIRTH_PRETERM#1336]\\n   +- Project [recode(INFANT_ALIVE_AT_REPORT#864, YUN) AS INFANT_ALIVE_AT_REPORT#1331, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_BEFORE#1026, CIG_1_TRI#1046, CIG_2_TRI#1066, CIG_3_TRI#1086, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, recode(DIABETES_PRE#883, YUN) AS DIABETES_PRE#1332, recode(DIABETES_GEST#884, YUN) AS DIABETES_GEST#1333, recode(HYP_TENS_PRE#885, YUN) AS HYP_TENS_PRE#1334, recode(HYP_TENS_GEST#886, YUN) AS HYP_TENS_GEST#1335, recode(PREV_BIRTH_PRETERM#887, YUN) AS PREV_BIRTH_PRETERM#1336]\\n      +- Project [INFANT_ALIVE_AT_REPORT#864, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_BEFORE#1026, CIG_1_TRI#1046, CIG_2_TRI#1066, CASE WHEN NOT (cast(CIG_3_TRI#877 as int) = 99) THEN CIG_3_TRI#877 ELSE cast(0 as string) END AS CIG_3_TRI#1086, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, DIABETES_PRE#883, DIABETES_GEST#884, HYP_TENS_PRE#885, HYP_TENS_GEST#886, PREV_BIRTH_PRETERM#887]\\n         +- Project [INFANT_ALIVE_AT_REPORT#864, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_BEFORE#1026, CIG_1_TRI#1046, CASE WHEN NOT (cast(CIG_2_TRI#876 as int) = 99) THEN CIG_2_TRI#876 ELSE cast(0 as string) END AS CIG_2_TRI#1066, CIG_3_TRI#877, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, DIABETES_PRE#883, DIABETES_GEST#884, HYP_TENS_PRE#885, HYP_TENS_GEST#886, PREV_BIRTH_PRETERM#887]\\n            +- Project [INFANT_ALIVE_AT_REPORT#864, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_BEFORE#1026, CASE WHEN NOT (cast(CIG_1_TRI#875 as int) = 99) THEN CIG_1_TRI#875 ELSE cast(0 as string) END AS CIG_1_TRI#1046, CIG_2_TRI#876, CIG_3_TRI#877, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, DIABETES_PRE#883, DIABETES_GEST#884, HYP_TENS_PRE#885, HYP_TENS_GEST#886, PREV_BIRTH_PRETERM#887]\\n               +- Project [INFANT_ALIVE_AT_REPORT#864, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CASE WHEN NOT (cast(CIG_BEFORE#874 as int) = 99) THEN CIG_BEFORE#874 ELSE cast(0 as string) END AS CIG_BEFORE#1026, CIG_1_TRI#875, CIG_2_TRI#876, CIG_3_TRI#877, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, DIABETES_PRE#883, DIABETES_GEST#884, HYP_TENS_PRE#885, HYP_TENS_GEST#886, PREV_BIRTH_PRETERM#887]\\n                  +- Project [INFANT_ALIVE_AT_REPORT#864, BIRTH_YEAR#865, BIRTH_PLACE#867, MOTHER_AGE_YEARS#868, FATHER_COMBINED_AGE#871, CIG_BEFORE#874, CIG_1_TRI#875, CIG_2_TRI#876, CIG_3_TRI#877, MOTHER_HEIGHT_IN#878, MOTHER_PRE_WEIGHT#880, MOTHER_DELIVERY_WEIGHT#881, MOTHER_WEIGHT_GAIN#882, DIABETES_PRE#883, DIABETES_GEST#884, HYP_TENS_PRE#885, HYP_TENS_GEST#886, PREV_BIRTH_PRETERM#887]\\n                     +- Relation[INFANT_ALIVE_AT_REPORT#864,BIRTH_YEAR#865,BIRTH_MONTH#866,BIRTH_PLACE#867,MOTHER_AGE_YEARS#868,MOTHER_RACE_6CODE#869,MOTHER_EDUCATION#870,FATHER_COMBINED_AGE#871,FATHER_EDUCATION#872,MONTH_PRECARE_RECODE#873,CIG_BEFORE#874,CIG_1_TRI#875,CIG_2_TRI#876,CIG_3_TRI#877,MOTHER_HEIGHT_IN#878,MOTHER_BMI_RECODE#879,MOTHER_PRE_WEIGHT#880,MOTHER_DELIVERY_WEIGHT#881,MOTHER_WEIGHT_GAIN#882,DIABETES_PRE#883,DIABETES_GEST#884,HYP_TENS_PRE#885,HYP_TENS_GEST#886,PREV_BIRTH_PRETERM#887,... 30 more fields] csv\\n\""
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.linalg as ln\n",
    "for cat in categorical_cols[1:]:\n",
    "    agg = births_transformed.groupBy('INFANT_ALIVE_AT_REPORT').pivot(cat).count()\n",
    "    print(agg.show())\n",
    "    agg_rdd =agg.rdd.map(lambda row:(row[1:])).flatMap(lambda row: [0 if e ==None else e for e in row]).collect()\n",
    "    print(agg_rdd)\n",
    "    row_length = len(agg.collect()[0])-1\n",
    "    agg =ln.Matrices.dense(row_length,2,agg_rdd)\n",
    "    test = st.Statistics.chiSqTest(agg)\n",
    "    print(cat,round(test.pValue,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[ 1.,  4.],\n",
      "             [ 2.,  5.],\n",
      "             [ 3.,  6.]])\n"
     ]
    }
   ],
   "source": [
    "print(ln.Matrices.dense(3,2,[1,2,3,4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.feature as  tf\n",
    "import pyspark.mllib.regression as reg\n",
    "import pyspark.mllib.linalg as ln\n",
    "hashing =tf.HashingTF(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "births_hashed = births_transformed.rdd.map(lambda row:[list(hashing.transform(row[1]).toArray()) if col =='BIRTH_PLACE' else row[i] for i,col in enumerate(features_to_keep)]).map(lambda row :[[e] if type(e)== int else e for e in row]).map(lambda row :[item for sublist in row for item in sublist]).map(lambda row :reg.LabeledPoint(row[0],ln.Vectors.dense(row[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "births_train,births_test=births_hashed.randomSplit([0.6,0.4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27268\n"
     ]
    }
   ],
   "source": [
    "print(births_train.count())\n",
    "#print(len(births_train.first()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1795.trainLogisticRegressionModelWithLBFGS.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 121.0 failed 1 times, most recent failure: Lost task 0.0 in stage 121.0 (TID 7086, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: Dimensions mismatch when adding new sample. Expecting 22 but got 23.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.add(MultivariateOnlineSummarizer.scala:87)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$15.apply(LogisticRegression.scala:509)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$15.apply(LogisticRegression.scala:508)\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:517)\n\tat org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS.runWithMlLogisticRegression$1(LogisticRegression.scala:453)\n\tat org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS.run(LogisticRegression.scala:459)\n\tat org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS.run(LogisticRegression.scala:425)\n\tat org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS.run(LogisticRegression.scala:355)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainRegressionModel(PythonMLLibAPI.scala:92)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainLogisticRegressionModelWithLBFGS(PythonMLLibAPI.scala:308)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Dimensions mismatch when adding new sample. Expecting 22 but got 23.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.add(MultivariateOnlineSummarizer.scala:87)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$15.apply(LogisticRegression.scala:509)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$15.apply(LogisticRegression.scala:508)\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-1c2dcfd05dc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegressionWithLBFGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionWithLBFGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbirths_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/bigdata/spark22/python/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, data, iterations, initialWeights, regParam, regType, intercept, corrections, tolerance, validateData, numClasses)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                     \u001b[0minitialWeights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumClasses\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/spark22/python/pyspark/mllib/regression.py\u001b[0m in \u001b[0;36m_regression_train_wrapper\u001b[0;34m(train_func, modelClass, data, initial_weights)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodelClass\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         weights, intercept, numFeatures, numClasses = train_func(\n\u001b[0;32m--> 215\u001b[0;31m             data, _convert_to_vector(initial_weights))\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodelClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/spark22/python/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rdd, i)\u001b[0m\n\u001b[1;32m    386\u001b[0m             return callMLlibFunc(\"trainLogisticRegressionModelWithLBFGS\", rdd, int(iterations), i,\n\u001b[1;32m    387\u001b[0m                                  \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m                                  float(tolerance), bool(validateData), int(numClasses))\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitialWeights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/spark22/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/spark22/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/spark22/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/spark22/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/spark22/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1795.trainLogisticRegressionModelWithLBFGS.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 121.0 failed 1 times, most recent failure: Lost task 0.0 in stage 121.0 (TID 7086, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: Dimensions mismatch when adding new sample. Expecting 22 but got 23.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.add(MultivariateOnlineSummarizer.scala:87)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$15.apply(LogisticRegression.scala:509)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$15.apply(LogisticRegression.scala:508)\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:517)\n\tat org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS.runWithMlLogisticRegression$1(LogisticRegression.scala:453)\n\tat org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS.run(LogisticRegression.scala:459)\n\tat org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS.run(LogisticRegression.scala:425)\n\tat org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS.run(LogisticRegression.scala:355)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainRegressionModel(PythonMLLibAPI.scala:92)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainLogisticRegressionModelWithLBFGS(PythonMLLibAPI.scala:308)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Dimensions mismatch when adding new sample. Expecting 22 but got 23.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.add(MultivariateOnlineSummarizer.scala:87)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$15.apply(LogisticRegression.scala:509)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$15.apply(LogisticRegression.scala:508)\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "lr_model = LogisticRegressionWithLBFGS.train(births_train,iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1878.fitChiSqSelector.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 126.0 failed 1 times, most recent failure: Lost task 0.0 in stage 126.0 (TID 7091, localhost, executor driver): java.lang.IndexOutOfBoundsException: 21 not in [-21,21)\n\tat breeze.linalg.DenseVector$mcD$sp.apply$mcD$sp(DenseVector.scala:72)\n\tat breeze.linalg.DenseVector$mcD$sp.apply(DenseVector.scala:71)\n\tat breeze.linalg.DenseVector$mcD$sp.apply(DenseVector.scala:53)\n\tat breeze.linalg.TensorLike$class.apply$mcID$sp(Tensor.scala:107)\n\tat breeze.linalg.DenseVector.apply$mcID$sp(DenseVector.scala:53)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1$$anonfun$apply$3.apply(ChiSqTest.scala:119)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1$$anonfun$apply$3.apply(ChiSqTest.scala:118)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.Range.foreach(Range.scala:160)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1.apply(ChiSqTest.scala:118)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1.apply(ChiSqTest.scala:102)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:373)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:373)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:372)\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1204)\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1204)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.countByValue(RDD.scala:1203)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$.chiSquaredFeatures(ChiSqTest.scala:124)\n\tat org.apache.spark.mllib.stat.Statistics$.chiSqTest(Statistics.scala:176)\n\tat org.apache.spark.mllib.feature.ChiSqSelector.fit(ChiSqSelector.scala:257)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.fitChiSqSelector(PythonMLLibAPI.scala:652)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IndexOutOfBoundsException: 21 not in [-21,21)\n\tat breeze.linalg.DenseVector$mcD$sp.apply$mcD$sp(DenseVector.scala:72)\n\tat breeze.linalg.DenseVector$mcD$sp.apply(DenseVector.scala:71)\n\tat breeze.linalg.DenseVector$mcD$sp.apply(DenseVector.scala:53)\n\tat breeze.linalg.TensorLike$class.apply$mcID$sp(Tensor.scala:107)\n\tat breeze.linalg.DenseVector.apply$mcID$sp(DenseVector.scala:53)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1$$anonfun$apply$3.apply(ChiSqTest.scala:119)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1$$anonfun$apply$3.apply(ChiSqTest.scala:118)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.Range.foreach(Range.scala:160)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1.apply(ChiSqTest.scala:118)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1.apply(ChiSqTest.scala:102)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-c6b718622491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChiSqSelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbirths_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/bigdata/spark22/python/pyspark/mllib/feature.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \"\"\"\n\u001b[1;32m    394\u001b[0m         jmodel = callMLlibFunc(\"fitChiSqSelector\", self.selectorType, self.numTopFeatures,\n\u001b[0;32m--> 395\u001b[0;31m                                self.percentile, self.fpr, self.fdr, self.fwe, data)\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mChiSqSelectorModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/spark22/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/spark22/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/spark22/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/spark22/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/spark22/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1878.fitChiSqSelector.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 126.0 failed 1 times, most recent failure: Lost task 0.0 in stage 126.0 (TID 7091, localhost, executor driver): java.lang.IndexOutOfBoundsException: 21 not in [-21,21)\n\tat breeze.linalg.DenseVector$mcD$sp.apply$mcD$sp(DenseVector.scala:72)\n\tat breeze.linalg.DenseVector$mcD$sp.apply(DenseVector.scala:71)\n\tat breeze.linalg.DenseVector$mcD$sp.apply(DenseVector.scala:53)\n\tat breeze.linalg.TensorLike$class.apply$mcID$sp(Tensor.scala:107)\n\tat breeze.linalg.DenseVector.apply$mcID$sp(DenseVector.scala:53)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1$$anonfun$apply$3.apply(ChiSqTest.scala:119)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1$$anonfun$apply$3.apply(ChiSqTest.scala:118)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.Range.foreach(Range.scala:160)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1.apply(ChiSqTest.scala:118)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1.apply(ChiSqTest.scala:102)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:373)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:373)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:372)\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1204)\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1204)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.countByValue(RDD.scala:1203)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$.chiSquaredFeatures(ChiSqTest.scala:124)\n\tat org.apache.spark.mllib.stat.Statistics$.chiSqTest(Statistics.scala:176)\n\tat org.apache.spark.mllib.feature.ChiSqSelector.fit(ChiSqSelector.scala:257)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.fitChiSqSelector(PythonMLLibAPI.scala:652)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IndexOutOfBoundsException: 21 not in [-21,21)\n\tat breeze.linalg.DenseVector$mcD$sp.apply$mcD$sp(DenseVector.scala:72)\n\tat breeze.linalg.DenseVector$mcD$sp.apply(DenseVector.scala:71)\n\tat breeze.linalg.DenseVector$mcD$sp.apply(DenseVector.scala:53)\n\tat breeze.linalg.TensorLike$class.apply$mcID$sp(Tensor.scala:107)\n\tat breeze.linalg.DenseVector.apply$mcID$sp(DenseVector.scala:53)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1$$anonfun$apply$3.apply(ChiSqTest.scala:119)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1$$anonfun$apply$3.apply(ChiSqTest.scala:118)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.Range.foreach(Range.scala:160)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1.apply(ChiSqTest.scala:118)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1.apply(ChiSqTest.scala:102)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "selector = tf.ChiSqSelector(4).fit(births_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wget http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz\n",
    "\n",
    "labelx = [('INFANT_ALIVE_AT_REPORT',typ.IntegerType()),\n",
    "        ('BIRTH_PLACE',typ.StringType()),\n",
    "        ('MOTHER_AGE_YEARS',typ.IntegerType()),\n",
    "        ('FATHER_COMBINED_AGE',typ.IntegerType()),\n",
    "        ('CIG_BEFORE',typ.IntegerType()),\n",
    "        ('CIG_1_TRI',typ.IntegerType()),\n",
    "        ('CIG_2_TRI',typ.IntegerType()),\n",
    "        ('CIG_3_TRI',typ.IntegerType()),\n",
    "         ('MOTHER_HEIGHT_IN',typ.IntegerType()),\n",
    "         ('MOTHER_PRE_WEIGHT',typ.IntegerType()),\n",
    "         ('MOTHER_DELIVERY_WEIGHT',typ.IntegerType()),\n",
    "         ('MOTHER_WEIGHT_GAIN',typ.IntegerType()),\n",
    "         ('DIABETES_PRE',typ.IntegerType()),\n",
    "         ('DIABETES_GEST',typ.IntegerType()),\n",
    "         ('HYP_TENS_PRE',typ.IntegerType()),\n",
    "         ('HYP_TENS_GEST',typ.IntegerType()),\n",
    "         ('PREV_BIRTH_PRETERM',typ.IntegerType())]\n",
    "shema = typ.StructType([typ.StructField(e[0],e[1],False) for e in labelx])\n",
    "data = spark.read.csv('file:///home/fordl/births_transformed.csv.gz',header=True,schema=shema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.withColumn(\"BIRTH_PLACE_INT\",data['BIRTH_PLACE'].cast(typ.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|BIRTH_PLACE|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "+-----------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+----------------------+\n",
      "|INFANT_ALIVE_AT_REPORT|\n",
      "+----------------------+\n",
      "|                     0|\n",
      "|                     0|\n",
      "|                     0|\n",
      "|                     0|\n",
      "|                     0|\n",
      "+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"BIRTH_PLACE\").show(4)\n",
    "type(data.select('BIRTH_PLACE'))\n",
    "data.select(\"INFANT_ALIVE_AT_REPORT\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = tf.OneHotEncoder(inputCol = 'BIRTH_PLACE_INT',outputCol='BIRHT_PLACE_VEC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureCreator = tf.VectorAssembler(inputCols=[col[0] for col in labels[2:]] + [encoder.getOutputCol()],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.classification as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic = cl.LogisticRegression(maxIter=10,regParam=0,labelCol=\"INFANT_ALIVE_AT_REPORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[encoder,featureCreator,logistic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_train,d_test = data.randomSplit([0.7,0.3],seed=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(d_train)\n",
    "test_model = model.transform(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE='1', MOTHER_AGE_YEARS=13, FATHER_COMBINED_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=66, MOTHER_PRE_WEIGHT=133, MOTHER_DELIVERY_WEIGHT=135, MOTHER_WEIGHT_GAIN=2, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0, BIRTH_PLACE_INT=1, BIRHT_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 13.0, 1: 99.0, 6: 66.0, 7: 133.0, 8: 135.0, 9: 2.0, 16: 1.0}), rawPrediction=DenseVector([1.1516, -1.1516]), probability=DenseVector([0.7598, 0.2402]), prediction=0.0)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.evaluation as ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='probability',labelCol='INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.741891349641666\n",
      "0.7151539716088794\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(test_model,{evaluator.metricName:'areaUnderROC'}))\n",
    "print(evaluator.evaluate(test_model,{evaluator.metricName:'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE='1', MOTHER_AGE_YEARS=13, FATHER_COMBINED_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=66, MOTHER_PRE_WEIGHT=133, MOTHER_DELIVERY_WEIGHT=135, MOTHER_WEIGHT_GAIN=2, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0, BIRTH_PLACE_INT=1, BIRHT_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 13.0, 1: 99.0, 6: 66.0, 7: 133.0, 8: 135.0, 9: 2.0, 16: 1.0}), rawPrediction=DenseVector([1.1516, -1.1516]), probability=DenseVector([0.7598, 0.2402]), prediction=0.0),\n",
       " Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE='1', MOTHER_AGE_YEARS=14, FATHER_COMBINED_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=63, MOTHER_PRE_WEIGHT=93, MOTHER_DELIVERY_WEIGHT=100, MOTHER_WEIGHT_GAIN=0, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0, BIRTH_PLACE_INT=1, BIRHT_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 14.0, 1: 99.0, 6: 63.0, 7: 93.0, 8: 100.0, 16: 1.0}), rawPrediction=DenseVector([1.0004, -1.0004]), probability=DenseVector([0.7311, 0.2689]), prediction=0.0)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型保存\n",
    "pipelinepath = './ml_lr_pipeline'\n",
    "pipeline.write().overwrite().save(pipelinepath)\n",
    "loadedpipeline = Pipeline.load(pipelinepath)\n",
    "loadedpipeline.fit(d_train).transform(d_test).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE='1', MOTHER_AGE_YEARS=13, FATHER_COMBINED_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=66, MOTHER_PRE_WEIGHT=133, MOTHER_DELIVERY_WEIGHT=135, MOTHER_WEIGHT_GAIN=2, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0, BIRTH_PLACE_INT=1, BIRHT_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 13.0, 1: 99.0, 6: 66.0, 7: 133.0, 8: 135.0, 9: 2.0, 16: 1.0}), rawPrediction=DenseVector([1.1516, -1.1516]), probability=DenseVector([0.7598, 0.2402]), prediction=0.0), Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE='1', MOTHER_AGE_YEARS=14, FATHER_COMBINED_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=63, MOTHER_PRE_WEIGHT=93, MOTHER_DELIVERY_WEIGHT=100, MOTHER_WEIGHT_GAIN=0, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0, BIRTH_PLACE_INT=1, BIRHT_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 14.0, 1: 99.0, 6: 63.0, 7: 93.0, 8: 100.0, 16: 1.0}), rawPrediction=DenseVector([1.0004, -1.0004]), probability=DenseVector([0.7311, 0.2689]), prediction=0.0), Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE='1', MOTHER_AGE_YEARS=14, FATHER_COMBINED_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=63, MOTHER_PRE_WEIGHT=106, MOTHER_DELIVERY_WEIGHT=115, MOTHER_WEIGHT_GAIN=9, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0, BIRTH_PLACE_INT=1, BIRHT_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 14.0, 1: 99.0, 6: 63.0, 7: 106.0, 8: 115.0, 9: 9.0, 16: 1.0}), rawPrediction=DenseVector([0.7542, -0.7542]), probability=DenseVector([0.6801, 0.3199]), prediction=0.0)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "modelpath = './pipemodel'\n",
    "model.write().overwrite().save(modelpath)\n",
    "loadedPipelineModel = PipelineModel.load(modelpath)\n",
    "test_reloadmodel = loadedPipelineModel.transform(d_test)\n",
    "print(test_reloadmodel.take(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic = cl.LogisticRegression(labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "grid = tune.ParamGridBuilder().addGrid(logistic.maxIter,[2,10,50]).addGrid(logistic.regParam,[0.01,0.05,0.3]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='probability',labelCol='INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = tune.CrossValidator(estimator = logistic,estimatorParamMaps =grid,evaluator=evaluator)\n",
    "pipeline =Pipeline(stages=[encoder,featureCreator])\n",
    "data_transformer = pipeline.fit(d_train)\n",
    "cv_model = cv.fit(data_transformer.transform(d_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7404526641072416\n",
      "0.7157767684747429\n"
     ]
    }
   ],
   "source": [
    "data_train = data_transformer.transform(d_test)\n",
    "results = cv_model.transform(data_train)\n",
    "print(evaluator.evaluate(results,{evaluator.metricName:'areaUnderROC'}))\n",
    "print(evaluator.evaluate(results,{evaluator.metricName:'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'maxIter': 50}, {'regParam': 0.01}], 0.738652833807851)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results =[(\n",
    "    [ {key.name:paramValue}\n",
    "    for key,paramValue\n",
    "    in zip (params.keys(),\n",
    "           params.values())],metric)\n",
    "    for params,metric in zip(cv_model.getEstimatorParamMaps(),\n",
    "                            cv_model.avgMetrics)]\n",
    "\n",
    "sorted(results,key=lambda el:el[1], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selector =tf.ChiSqSelector(numTopFeatures=5,featuresCol=featureCreator.getOutputCol(),\n",
    "                          outputCol='selectedFeatures',labelCol='INFANT_ALIVE_AT_REPORT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic = cl.LogisticRegression(labelCol='INFANT_ALIVE_AT_REPORT',featuresCol='selectedFeatures')\n",
    "pipeline = Pipeline(stages=[encoder,featureCreator,selector])\n",
    "data_transformer= pipeline.fit(d_train)\n",
    "tvs = tune.TrainValidationSplit(estimator=logistic,estimatorParamMaps=grid,evaluator=evaluator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7294296314442145\n",
      "0.7037759446410553\n"
     ]
    }
   ],
   "source": [
    "tvsmodel=tvs.fit(data_transformer.transform(d_train))\n",
    "data_train = data_transformer.transform(d_test)\n",
    "results = tvsmodel.transform(data_train)\n",
    "print(evaluator.evaluate(results,{evaluator.metricName:'areaUnderROC'}))\n",
    "print(evaluator.evaluate(results,{evaluator.metricName:'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luokui/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
